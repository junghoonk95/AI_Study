{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dNRuTQQpjGmn",
        "outputId": "020190ab-07ad-4f04-e827-02e5d542957e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KoGPT2-chatbot'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 100 (delta 16), reused 15 (delta 15), pack-reused 78\u001b[K\n",
            "Receiving objects: 100% (100/100), 113.50 KiB | 14.19 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Submodule 'Chatbot_data' (https://github.com/haven-jeon/Chatbot_data.git) registered for path 'Chatbot_data'\n",
            "Cloning into '/content/KoGPT2-chatbot/Chatbot_data'...\n",
            "remote: Enumerating objects: 24, done.        \n",
            "remote: Counting objects: 100% (4/4), done.        \n",
            "remote: Compressing objects: 100% (4/4), done.        \n",
            "remote: Total 24 (delta 0), reused 3 (delta 0), pack-reused 20        \n",
            "Submodule path 'Chatbot_data': checked out '235fac5aea3badab22743f7048afe936cf72f822'\n"
          ]
        }
      ],
      "source": [
        "# KoGPT2-chatbot 소스 코드 복사\n",
        "!git clone --recurse-submodules https://github.com/haven-jeon/KoGPT2-chatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 폴더 이동\n",
        "%cd KoGPT2-chatbot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ziKn3Fw1jUNA",
        "outputId": "1a9e2870-8a9c-406f-9c2a-85128eedd75c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KoGPT2-chatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pytorch-lightning\n",
        "!pip install pytorch_lightning==1.2.10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ylf3BwbBm2ju",
        "outputId": "0c843695-11d6-4ce2-ecde-020bd34a5406"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping pytorch-lightning as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning==1.2.10\n",
            "  Downloading pytorch_lightning-1.2.10-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.9/841.9 KB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10) (2.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10) (21.3)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.3.tar.gz (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 KB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torchmetrics==0.2.0\n",
            "  Downloading torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10) (2022.11.0)\n",
            "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10) (6.0)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10) (4.64.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (3.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (2.25.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (2.16.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.51.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (0.4.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (3.19.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (3.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4->pytorch_lightning==1.2.10) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->pytorch_lightning==1.2.10) (3.0.9)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (1.8.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (5.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (3.2.2)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492036 sha256=6b16d36f82df105d2f499458a4d6edf3fe7f47618cdcb0bf599ad9a04c7b5643\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/0b/ee/e6994fadb42c1354dcccb139b0bf2795271bddfe6253ccdf11\n",
            "Successfully built future\n",
            "Installing collected packages: future, torchmetrics, pytorch_lightning\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed future-0.18.3 pytorch_lightning-1.2.10 torchmetrics-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aJQsU8g8l3AW",
        "outputId": "db6f2205-38a6-4e9c-c2db-c22d150ce98c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchtext 0.14.1\n",
            "Uninstalling torchtext-0.14.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.8/dist-packages/torchtext-0.14.1.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/torchtext/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torchtext-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.10.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qfbQCoyRl42e",
        "outputId": "99a6df2c-c674-4774-e31f-c986d776ae19"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.1\n",
            "  Downloading torchtext-0.10.1-cp38-cp38-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.1) (2.25.1)\n",
            "Collecting torch==1.9.1\n",
            "  Downloading torch-1.9.1-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.1) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.9.1->torchtext==0.10.1) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.1) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.1) (4.0.0)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.9.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.9.1 torchtext-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TsIJL7t0nIT3",
        "outputId": "17d3ca82-eb40-41ac-b372-a059f752f35a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전훈련된 KoGPT2를 챗봇 데이터로 파인튜닝\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --train --gpus 1 --max_epochs 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZkC2d73FkMVc",
        "outputId": "1266f864-33a3-45d8-f7d5-c866dfb7737f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading (…)/main/tokenizer.json: 100% 2.83M/2.83M [00:01<00:00, 2.12MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 1.00k/1.00k [00:00<00:00, 390kB/s]\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=96, benchmark=False, chat=False, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=5e-05, max_epochs=2, max_len=32, max_steps=None, min_epochs=None, min_steps=None, model_params='model_chp/model_-last.ckpt', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sentiment='0', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tpu_cores=None, track_grad_norm=-1, train=True, truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 513M/513M [00:09<00:00, 54.6MB/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | kogpt2        | GPT2LMHeadModel  | 125 M \n",
            "1 | loss_function | CrossEntropyLoss | 0     \n",
            "---------------------------------------------------\n",
            "125 M     Trainable params\n",
            "0         Non-trainable params\n",
            "125 M     Total params\n",
            "500.656   Total estimated model params size (MB)\n",
            "Epoch 0:   0% 0/124 [00:00<?, ?it/s] INFO:root:contexts : 수백번 싸우고 수백번 화해\n",
            "INFO:root:contexts : 전과하고 재미있게 학교 다니고 싶다.\n",
            "INFO:root:toked ctx: ['<usr>', '▁수백', '번', '▁싸', '우고', '▁수백', '번', '▁화해', '<unused1>', '▁1']\n",
            "INFO:root:toked ctx: ['<usr>', '▁전과', '하고', '▁재미', '있게', '▁학교', '▁다니', '고', '▁싶', '다.', '<unused1>', '▁0']\n",
            "INFO:root:response : 너무 안맞으면 헤어지는 것이 서로에게 도움이 될지도 몰라요.\n",
            "INFO:root:response : 충분히 잘할 수 있을 거예요.\n",
            "INFO:root:toked response : ['<sys>', '▁충분히', '▁잘', '할', '▁수', '▁있을', '▁거', '예', '요.', '</s>']\n",
            "INFO:root:toked response : ['<sys>', '▁너무', '▁안', '맞', '으면', '▁헤', '어지는', '▁것이', '▁서로', '에게', '▁도움이', '▁될', '지도', '▁몰', '라', '요.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁충분히', '▁잘', '할', '▁수', '▁있을', '▁거', '예', '요.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁너무', '▁안', '맞', '으면', '▁헤', '어지는', '▁것이', '▁서로', '에게', '▁도움이', '▁될', '지도', '▁몰', '라', '요.', '</s>']\n",
            "Epoch 0: 100% 124/124 [02:20<00:00,  1.13s/it, loss=28.7, v_num=0]Epoch 0, global step 123: train_loss reached 32.81413 (best 32.81413), saving model to \"/content/KoGPT2-chatbot/model_chp/model_-epoch=00-train_loss=32.81.ckpt\" as top 1\n",
            "tcmalloc: large alloc 1180934144 bytes == 0xbb0d6000 @  0x7f19238f1680 0x7f1923911da2 0x5f714c 0x64d800 0x527022 0x5c4520 0x5f6eb7 0x7f1912a4c564 0x7f1912aaf48d 0x7f1900d86025 0x7f1900d8195e 0x7f1900d86afa 0x7f1912aaf9c2 0x7f191254f1ee 0x5f5b39 0x5f6706 0x50ba83 0x570b82 0x569d8a 0x5f60c3 0x56bab6 0x569d8a 0x5f60c3 0x570b82 0x5f5ee6 0x56bab6 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x50b3a0\n",
            "tcmalloc: large alloc 1476173824 bytes == 0x145a38000 @  0x7f19238f1680 0x7f1923911da2 0x5f714c 0x64d800 0x527022 0x5c4520 0x5f6eb7 0x7f1912a4c564 0x7f1912aaf48d 0x7f1900d86025 0x7f1900d8195e 0x7f1900d86afa 0x7f1912aaf9c2 0x7f191254f1ee 0x5f5b39 0x5f6706 0x50ba83 0x570b82 0x569d8a 0x5f60c3 0x56bab6 0x569d8a 0x5f60c3 0x570b82 0x5f5ee6 0x56bab6 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x50b3a0\n",
            "tcmalloc: large alloc 1845223424 bytes == 0x7f14ea042000 @  0x7f19238f1680 0x7f1923911da2 0x5f714c 0x64d800 0x527022 0x5c4520 0x5f6eb7 0x7f1912a4c564 0x7f1912aaf48d 0x7f1900d86025 0x7f1900d8195e 0x7f1900d86afa 0x7f1912aaf9c2 0x7f191254f1ee 0x5f5b39 0x5f6706 0x50ba83 0x570b82 0x569d8a 0x5f60c3 0x56bab6 0x569d8a 0x5f60c3 0x570b82 0x5f5ee6 0x56bab6 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x50b3a0\n",
            "tcmalloc: large alloc 1845223424 bytes == 0x7f14ea042000 @  0x7f19238f1680 0x7f1923911da2 0x5f714c 0x64d800 0x527022 0x5c4520 0x5f6eb7 0x7f1912a4c564 0x7f1912aaf48d 0x7f1900d86025 0x7f1900d8195e 0x7f1900d86afa 0x7f1912aaf9c2 0x7f191254f1ee 0x5f5b39 0x5f6706 0x50ba83 0x570b82 0x569d8a 0x5f60c3 0x56bab6 0x569d8a 0x5f60c3 0x570b82 0x5f5ee6 0x56bab6 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x50b3a0\n",
            "Epoch 1:   0% 0/124 [00:00<?, ?it/s, loss=28.7, v_num=0]INFO:root:contexts : 연애 하고 싶어\n",
            "INFO:root:toked ctx: ['<usr>', '▁연애', '▁하고', '▁싶어', '<unused1>', '▁0']\n",
            "INFO:root:response : 잘할 수 있을 거니 걱정마세요.\n",
            "INFO:root:toked response : ['<sys>', '▁잘', '할', '▁수', '▁있을', '▁거', '니', '▁걱정', '마', '세', '요.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁잘', '할', '▁수', '▁있을', '▁거', '니', '▁걱정', '마', '세', '요.', '</s>']\n",
            "INFO:root:contexts : 놀이공원 알바 재밌대\n",
            "INFO:root:toked ctx: ['<usr>', '▁놀이', '공원', '▁알바', '▁재', '밌', '대', '<unused1>', '▁0']\n",
            "INFO:root:response : 특별한 경험이 될 것 같아요.\n",
            "INFO:root:toked response : ['<sys>', '▁특별한', '▁경험이', '▁될', '▁것', '▁같아', '요.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁특별한', '▁경험이', '▁될', '▁것', '▁같아', '요.', '</s>']\n",
            "Epoch 1: 100% 124/124 [02:20<00:00,  1.14s/it, loss=27.1, v_num=0]Epoch 1, global step 247: train_loss reached 25.97167 (best 25.97167), saving model to \"/content/KoGPT2-chatbot/model_chp/model_-epoch=01-train_loss=25.97.ckpt\" as top 1\n",
            "tcmalloc: large alloc 1845223424 bytes == 0x7f14ea042000 @  0x7f19238f1680 0x7f1923911da2 0x5f714c 0x64d800 0x527022 0x5c4520 0x5f6eb7 0x7f1912a4c564 0x7f1912aaf48d 0x7f1900d86025 0x7f1900d8195e 0x7f1900d86afa 0x7f1912aaf9c2 0x7f191254f1ee 0x5f5b39 0x5f6706 0x50ba83 0x570b82 0x569d8a 0x5f60c3 0x56bab6 0x569d8a 0x5f60c3 0x570b82 0x5f5ee6 0x56bab6 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x50b3a0\n",
            "Saving latest checkpoint...\n",
            "Epoch 1: 100% 124/124 [02:38<00:00,  1.28s/it, loss=27.1, v_num=0]\n",
            "INFO:root:best model path /content/KoGPT2-chatbot/model_chp/model_-epoch=01-train_loss=25.97.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 대화 테스트, `quit`를 입력하면 대화를 종료합니다.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --gpus 1 --chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bxz4sB5QmZy7",
        "outputId": "1a8febe0-cb3b-4afe-d527-d64ba9654613"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=96, benchmark=False, chat=True, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=5e-05, max_epochs=None, max_len=32, max_steps=None, min_epochs=None, min_steps=None, model_params='model_chp/model_-last.ckpt', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sentiment='0', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tpu_cores=None, track_grad_norm=-1, train=False, truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "user > 배고파\n",
            "파\n",
            "Simsimi > 맛난거 드세요.\n",
            "user > Simsimi > 맛있게 드세요.\n",
            "user > 배고파!\n",
            "Simsimi > 맛있는 거 드세요.\n",
            "user > 안녕\n",
            "Simsimi > 안녕.\n",
            "user > 녕\n",
            "Simsimi > 안녕.\n",
            "user > 가스비 너무 많이 나왔다 \n",
            "Simsimi > 절약하면 할 수 있어요.\n",
            "user > quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_PS4sauptym"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}